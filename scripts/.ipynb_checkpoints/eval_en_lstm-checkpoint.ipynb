{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from pandas import *\n",
    "import torch.nn.functional as F\n",
    "import dictionary_corpus\n",
    "from torch.autograd import Variable\n",
    "from model import RNNModel\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1111)\n",
    "np.random.seed(1111)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fn = \"../../../data/lm/English/hidden650_batch128_dropout0.2_lr20.0.pt\"\n",
    "model_name = \"Gulordava\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py:868: SourceChangeWarning: source code of class 'model.RNNModel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py:868: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py:868: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py:868: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.LSTM' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py:868: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "model_ = None\n",
    "with open(fn, \"rb\") as model_f:\n",
    "    model_ = torch.load(fn, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNModel(\"LSTM\", 50001, 650, 650, 2, 0.2, False)\n",
    "model.eval()\n",
    "model.load_state_dict(model_.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to data files\n",
    "data_path = \"../../../data/lm/English\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dictionary_corpus.Dictionary(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vocab(word_list):\n",
    "    unknown = set()\n",
    "    for w in word_list:\n",
    "        try:\n",
    "            idx = dictionary.word2idx[w]\n",
    "        except KeyError:\n",
    "            unknown.add(w)\n",
    "    print(unknown)\n",
    "    print(len(unknown), \"words are not in the model's vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_csv(filename):\n",
    "    data = read_csv(filename, delimiter = ';')\n",
    "    words = []\n",
    "    for (colname, colval) in data.iteritems():\n",
    "        if colname != \"item\":\n",
    "            for col in colval.values:\n",
    "                words_col = col.split()\n",
    "                for w in words_col:\n",
    "                    words.append(w)\n",
    "    check_vocab(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that assigns surprisal values to each word in a sentence given a previous context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_surprisal(prompt):\n",
    "    \"\"\"\n",
    "    prompt: list with words (including punctuation)\n",
    "    return: list with surprisal values\n",
    "    \"\"\"\n",
    "    np.random.seed(1111)\n",
    "    # prompt should be a list with words, punctuation and <eos>\n",
    "    #surprisal_arr = []  \n",
    "    surprisal_arr = [0]  # surprisal for initial position already added\n",
    "    indices = [dictionary.word2idx[w] if w in dictionary.word2idx\n",
    "               else dictionary.word2idx[\"<unk>\"]\n",
    "               for w in prompt]\n",
    "    indices = torch.tensor(indices, dtype=torch.long)\n",
    "    output, hidden = model(indices.view(-1, 1),  # Remember, (sequence_length, batch_size)\n",
    "                           model.init_hidden(1))  # one input at a time, thus batch_size = 1\n",
    "    #for position, next_word in enumerate(prompt[:-1]): \n",
    "    for position, next_word in enumerate(prompt[1:-1]):  # excluding surprisal at the first and last positions\n",
    "        current_word_scores = output[position].view(-1)  # the output vector corresponding to the current word\n",
    "        current_word_probs = F.log_softmax(current_word_scores, dim=0) # (log) softmax the score to get probabilities\n",
    "        next_word_prob = current_word_probs[dictionary.word2idx[next_word]] # get the probability of the true next word\n",
    "        surprisal = next_word_prob*(-1)  # item gives you an integer from a tensor that has one element\n",
    "        surprisal_arr.append(surprisal.item())\n",
    "    surprisal_arr.append(0)  # surprisal for <eos> given punctuation\n",
    "    # print(len(surprisal_arr) == len(prompt))  # True\n",
    "    return surprisal_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_surprisal_values(data):\n",
    "    surprisal_values = []\n",
    "    end_idx = data.loc[data['word'] == '<eos>'].index.to_list()  # list with idx of rows that contain <eos>\n",
    "    end_idx = [-1,*end_idx]  # inserting -1 as the start index to get the first sentence right\n",
    "    for i in range(len(end_idx)-1):\n",
    "        sent_range = range(end_idx[i]+1, end_idx[i+1]+1)\n",
    "        sent_words = data.iloc[sent_range]['word'].to_list()\n",
    "        surprisal_arr = sent_surprisal(sent_words)\n",
    "        for s in surprisal_arr:\n",
    "            surprisal_values.append(s)\n",
    "    return surprisal_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for the analysis of distance and syntactic position:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_from_dataset(dataset, model_name):\n",
    "    # result_filename = '../results' + dataset[17:-4] + '_result_' + model_name + '.csv'\n",
    "    result_filename = '../results' + dataset[17:-4] + '_result' + '.csv' # when working with one model\n",
    "    print(result_filename)\n",
    "    return result_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data(dataset, model_name):\n",
    "    words = []\n",
    "    data = read_csv(dataset, index_col=0)\n",
    "    for index, row in data.iterrows():\n",
    "        words.append(row['word'])\n",
    "    check_vocab(words)\n",
    "    surprisal_values = get_surprisal_values(data)\n",
    "    data['surprisal'] = surprisal_values\n",
    "    #unk = ['FALSE']*len(surprisal_values)\n",
    "    #data['unk'] = unk\n",
    "    #model = [model_name]*len(surprisal_values)\n",
    "    #data['model'] = model\n",
    "    data[\"dependency\"] = \"Wh\"\n",
    "    data[\"language\"] = \"English\"\n",
    "    result = filename_from_dataset(dataset, model_name)\n",
    "    data.to_csv(result, encoding=\"utf-8-sig\", index=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<bos>'}\n",
      "1 words are not in the model's vocabulary\n",
      "../results/jml_sentences/eq_subj_wh_en_result.csv\n",
      "{'<bos>'}\n",
      "1 words are not in the model's vocabulary\n",
      "../results/jml_sentences/whether_wh_en_result.csv\n",
      "{'<bos>'}\n",
      "1 words are not in the model's vocabulary\n",
      "../results/jml_sentences/subject_wh_en_result.csv\n",
      "{'<bos>'}\n",
      "1 words are not in the model's vocabulary\n",
      "../results/jml_sentences/unbound_wh_en_result.csv\n"
     ]
    }
   ],
   "source": [
    "eq = analyze_data('../test_sentences/jml_sentences/eq_subj_wh_en.csv', model_name)\n",
    "whether = analyze_data('../test_sentences/jml_sentences/whether_wh_en.csv', model_name)\n",
    "subject = analyze_data('../test_sentences/jml_sentences/subject_wh_en.csv', model_name)\n",
    "unbound = analyze_data('../test_sentences/jml_sentences/unbound_wh_en.csv', model_name)\n",
    "eq[\"dependency\"] = \"Wh\"\n",
    "eq[\"language\"] = \"English\"\n",
    "whether[\"dependency\"] = \"Wh\"\n",
    "whether[\"language\"] = \"English\"\n",
    "subject[\"dependency\"] = \"Wh\"\n",
    "subject[\"language\"] = \"English\"\n",
    "unbound[\"dependency\"] = \"Wh\"\n",
    "unbound[\"language\"] = \"English\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbound.to_csv('../results/unbound_result_en_jml.csv', encoding=\"utf-8-sig\", index=False)\n",
    "whether.to_csv('../results/whether_result_en_jml.csv', encoding=\"utf-8-sig\", index=False)\n",
    "subject.to_csv('../results/subject_result_en_jml.csv', encoding=\"utf-8-sig\", index=False)\n",
    "eq.to_csv('../results/eq_subj_result_en_jml.csv', encoding=\"utf-8-sig\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
